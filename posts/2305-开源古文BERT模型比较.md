---
title: 开源古文BERT模型比较
number: 201
url: https://github.com/King-of-Infinite-Space/thoughts/discussions/201
createdAt: 2023-05-19T04:30:38Z
lastEditedAt: null
updatedAt: 2023-05-19T04:30:39Z
author: King-of-Infinite-Space
category: 博文
labels:
  - 说文解字
  - 赛博空间
  - 网上冲浪
countZH: 320
countEN: 100
filename: 2305-开源古文BERT模型比较
---

近来想尝试用BERT (Bidirectional Encoder Representations from Transformers) 分析一下古诗词[^1]。搜索到一些开源模型，在此略作总结比较（大致按年份排序）。看下来SikuBERT提供的信息较为详细，可以一试。

|          | **BERT-CCPoem**                                    |
|----------|----------------------------------------------------|
| 链接     | [GH](https://github.com/THUNLP-AIPoet/BERT-CCPoem) |
| 文章     | 无                                                 |
| 尺寸     | medium                                             |
| 初始化   | 随机？                                             |
| 训练数据 | 9M句（0.9M首古诗）                                 |
| 词表大小 | 12K                                                |
| 团队背景 | 清华大学                                           |
| 发布时间 | 2020                                               |

|          | **GuwenBERT**                                                                                                                                                                                       |
|----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 链接     | [GH](https://github.com/ethan-yt/guwenbert) / [HF](https://huggingface.co/ethanyt/guwenbert-base)                                                                                                   |
| 文章     | [slides](https://github.com/Ethan-yt/guwenbert/blob/main/assets/%E5%9F%BA%E4%BA%8E%E7%BB%A7%E7%BB%AD%E8%AE%AD%E7%BB%83%E7%9A%84%E5%8F%A4%E6%B1%89%E8%AF%AD%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.pdf) |
| 尺寸     | base, large                                                                                                                                                                                         |
| 初始化   | RoBERTa-wwm-ext-Chinese                                                                                                                                                                             |
| 训练数据 | 1700M字（殆知阁古代文献，繁转简）                                                                                                                                                                   |
| 词表大小 | 23K                                                                                                                                                                                                 |
| 团队背景 | 北京理工大学                                                                                                                                                                                        |
| 发布时间 | 2020                                                                                                                                                                                                |

注：另有日本团队基于GuwenBERT开发的模型([HF](https://huggingface.co/KoichiYasuoka/roberta-classical-chinese-base-char))，但信息不全故不收录。

|          | **AnchiBERT**                              |
|----------|--------------------------------------------|
| 链接     | [GH](https://github.com/ttzHome/AnchiBERT) |
| 文章     | [arXiv](https://arxiv.org/abs/2009.11473)  |
| 尺寸     | base                                       |
| 初始化   | BERT-base-chinese                          |
| 训练数据 | 40M字（文史诗联）                          |
| 词表大小 | 21K                                        |
| 团队背景 | 四川大学                                   |
| 发布时间 | 2021                                       |

|          | **SikuBERT<br>SikuRoBERTa**                                                                                                                              |
|----------|----------------------------------------------------------------------------------------------------------------------------------------------------------|
| 链接     | [GH](https://github.com/hsc748NLP/SikuBERT-for-digital-humanities-and-classical-Chinese-information-processing) / [HF](https://huggingface.co/SIKU-BERT) |
| 文章     | [link](https://www.dhcn.cn/site/works/papers/type/nlp_ai/18536.html)                                                                                     |
| 尺寸     | base                                                                                                                                                     |
| 初始化   | BERT-base-chinese<br>RoBERTa-wwm-ext-Chinese                                                                                                             |
| 训练数据 | 500M字（四库全书，繁体）                                                                                                                                 |
| 词表大小 | 30K                                                                                                                                                      |
| 团队背景 | 南京农业大学                                                                                                                                             |
| 发布时间 | 2021                                                                                                                                                     |

|          | **BERT-ancient-chinese**                                                                                            |
|----------|---------------------------------------------------------------------------------------------------------------------|
| 链接     | [GH](https://github.com/Jihuai-wpy/bert-ancient-chinese) / [HF](https://huggingface.co/Jihuai/bert-ancient-chinese) |
| 文章     | 无？                                                                                                                |
| 尺寸     | base                                                                                                                |
| 初始化   | BERT-base-chinese                                                                                                   |
| 训练数据 | 3000M字？                                                                                                           |
| 词表大小 | 38K                                                                                                                 |
| 团队背景 | 复旦大学                                                                                                            |
| 发布时间 | 2022                                                                                                                |

----------

参考数值：

BERT-medium L=8, A=8, H=512  
BERT-base L=12, A=12, H=768  
BERT-large L=24, A=16, H=1024

BERT-base-chinese词表大小21K

<img src="https://count.lnfinite.space/post/bert-chc.svg?plus=1" width="0" height="0"/>

[^1]: 目前更关心理解而非生成，所以暂不考虑GPT系模型。（优质语料都读不完，何必生成更多良莠不齐的作品。）
